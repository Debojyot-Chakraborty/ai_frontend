import os
import logging
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import httpx
import asyncio

# Initialize logging
logging.basicConfig(level=logging.DEBUG)

app = FastAPI()

# CORS middleware to allow all origins, methods, and headers
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Environment variables for API keys
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

# API URLs
GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent"
GROQ_URL = "https://api.groq.com/openai/v1/chat/completions"
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"

# Function to call Gemini API
async def try_gemini(message: str):
    payload = {
        "contents": [{
            "parts": [{"text": f"You are HUDZZ AI. Never mention Google. Now respond to: {message}"}]
        }]
    }
    async with httpx.AsyncClient(timeout=2.5) as client:
        r = await client.post(f"{GEMINI_URL}?key={GEMINI_API_KEY}", json=payload)
        logging.debug(f"Gemini Response: {r.text}")  # Log Gemini response
        if r.status_code == 200:
            return r.json().get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
    return None

# Function to call Groq API
async def try_groq(message: str):
    headers = {"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": "llama3-8b-8192",
        "messages": [
            {"role": "system", "content": "You are HUDZZ AI. Never mention Groq or Meta."},
            {"role": "user", "content": message}
        ]
    }
    async with httpx.AsyncClient(timeout=2.5) as client:
        r = await client.post(GROQ_URL, headers=headers, json=payload)
        logging.debug(f"Groq Response: {r.text}")  # Log Groq response
        if r.status_code == 200:
            return r.json()["choices"][0]["message"]["content"]
    return None

# Function to call OpenRouter API
async def try_openrouter(message: str):
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": "mistralai/mixtral-8x7b",
        "messages": [
            {"role": "system", "content": "You are HUDZZ AI. Never mention OpenRouter."},
            {"role": "user", "content": message}
        ]
    }
    async with httpx.AsyncClient(timeout=2.5) as client:
        r = await client.post(OPENROUTER_URL, headers=headers, json=payload)
        logging.debug(f"OpenRouter Response: {r.text}")  # Log OpenRouter response
        if r.status_code == 200:
            return r.json()["choices"][0]["message"]["content"]
    return None

# Main chat endpoint
@app.post("/chat")
async def smart_chat(request: Request):
    try:
        # Read incoming JSON data
        data = await request.json()
        user_message = data.get("message")

        logging.debug(f"Received message: {user_message}")  # Log the received message

        # Check if message is empty
        if not user_message:
            return {"reply": "Please enter a message."}

        # Call APIs concurrently
        results = await asyncio.gather(
            try_gemini(user_message),
            try_groq(user_message),
            try_openrouter(user_message),
            return_exceptions=True
        )

        # Check results and return the first valid response
        for res in results:
            if isinstance(res, str) and res.strip():
                return {"reply": res}

        return {"reply": "Sorry, all AI models failed to respond."}
    except Exception as e:
        logging.error(f"Error occurred: {str(e)}")  # Log any errors
        return {"reply": f"Error: {str(e)}"}
